{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in .\\spacy\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in .\\spacy\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in .\\spacy\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in .\\spacy\\lib\\site-packages (from nltk) (2023.3.23)\n",
      "Requirement already satisfied: tqdm in .\\spacy\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in .\\spacy\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (e:\\spacey\\spacy\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (e:\\spacey\\spacy\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (e:\\spacey\\spacy\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (e:\\spacey\\spacy\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk\n",
    "! pip install gingerit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\keval\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\keval\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\keval\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase_sentence(sentence):\n",
    "    # Tokenize the sentence\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "    # Get the part of speech for each token\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Initialize an empty list to store the paraphrased sentence\n",
    "    paraphrased_sentence = []\n",
    "\n",
    "    for token, pos in pos_tags:\n",
    "        # If the token is a noun or a verb, try to find synonyms for it\n",
    "        if pos.startswith('N') or pos.startswith('V'):\n",
    "            synonyms = []\n",
    "            for syn in wordnet.synsets(token):\n",
    "                for lemma in syn.lemmas():\n",
    "                    synonyms.append(lemma.name())\n",
    "\n",
    "            # If synonyms were found, choose one at random and add it to the paraphrased sentence\n",
    "            if synonyms:\n",
    "                paraphrased_sentence.append(synonyms[0].replace('_', ' '))\n",
    "            # If no synonyms were found, add the original token to the paraphrased sentence\n",
    "            else:\n",
    "                paraphrased_sentence.append(token)\n",
    "        # If the token is not a noun or a verb, add it to the paraphrased sentence\n",
    "        else:\n",
    "            paraphrased_sentence.append(token)\n",
    "    # Join the paraphrased sentence and return it\n",
    "    return ' '.join(paraphrased_sentence)\n",
    "\n",
    "\n",
    "# Read in the input file\n",
    "with open('input.txt', 'r') as f:\n",
    "    input_text = f.read()\n",
    "\n",
    "# Paraphrase the input text\n",
    "paraphrased_text = []\n",
    "for sentence in nltk.sent_tokenize(input_text):\n",
    "    paraphrased_sentence = paraphrase_sentence(sentence)\n",
    "    paraphrased_text.append(paraphrased_sentence)\n",
    "paraphrased_text = ' '.join(paraphrased_text)\n",
    "\n",
    "str_en = paraphrased_text.encode(\"ascii\", \"ignore\")\n",
    "paraphrased_text = str_en.decode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gingerit.gingerit import GingerIt\n",
    "\n",
    "parser = GingerIt()\n",
    "parser.parse(paraphrased_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the output to a file\n",
    "with open('output.txt', 'w') as f:\n",
    "    f.write(paraphrased_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
